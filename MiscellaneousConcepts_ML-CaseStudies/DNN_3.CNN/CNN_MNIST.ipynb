{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print(K.image_data_format())\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before One hot encoding\n",
      "(60000,)\n",
      "(10000,)\n",
      "After One hot encoding\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Before One hot encoding\")\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(\"After One hot encoding\")\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-CONVLAYER CNN with Kernel 3X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying ConvNet\n",
    "\n",
    "model = Sequential()\n",
    "#--> 32 kernels of size 3X3 ,input_shape=(28,28,1)\n",
    "#--> conv->relu->maxpool->repeat...->Flatten->Fully Connected Dense->dropout->Dense (1 conv block)\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "469/469 [==============================] - 52s 110ms/step - loss: 0.2414 - accuracy: 0.9267 - val_loss: 0.0479 - val_accuracy: 0.9856\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 51s 109ms/step - loss: 0.0846 - accuracy: 0.9754 - val_loss: 0.0358 - val_accuracy: 0.9867\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0622 - accuracy: 0.9812 - val_loss: 0.0330 - val_accuracy: 0.9888\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0527 - accuracy: 0.9839 - val_loss: 0.0318 - val_accuracy: 0.9893\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0459 - accuracy: 0.9856 - val_loss: 0.0313 - val_accuracy: 0.9898\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0376 - accuracy: 0.9881 - val_loss: 0.0299 - val_accuracy: 0.9908\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0349 - accuracy: 0.9893 - val_loss: 0.0270 - val_accuracy: 0.9914\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 50s 106ms/step - loss: 0.0307 - accuracy: 0.9901 - val_loss: 0.0263 - val_accuracy: 0.9923\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 52s 110ms/step - loss: 0.0292 - accuracy: 0.9908 - val_loss: 0.0281 - val_accuracy: 0.9913\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 54s 115ms/step - loss: 0.0253 - accuracy: 0.9919 - val_loss: 0.0245 - val_accuracy: 0.9939\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 54s 115ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 0.0254 - val_accuracy: 0.9925\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 55s 116ms/step - loss: 0.0222 - accuracy: 0.9928 - val_loss: 0.0262 - val_accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faddf567d90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--->Compile -->fit -->Evaluate\n",
    "#loss=categorical_crossentropy(10 classes, optimitzer=Adam/Adadelta/GradientDescent, metrics=accuracy/r2score)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02622266300022602\n",
      "Test accuracy: 0.9925000071525574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Layer CNN\n",
    "- a simple stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYmVjGgwXaWN"
   },
   "outputs": [],
   "source": [
    "## Applying ConvNet\n",
    "\n",
    "# model = Sequential()\n",
    "# #--> 32 kernels of size 3X3 ,input_shape=(28,28,1)\n",
    "# #--> conv->relu->maxpool->repeat...->Flatten->Fully Connected Dense->dropout->Dense (1 conv block)\n",
    "# model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.25))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.6058 - accuracy: 0.8076 - val_loss: 0.1446 - val_accuracy: 0.9521\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.2072 - accuracy: 0.9393 - val_loss: 0.0984 - val_accuracy: 0.9690\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.1566 - accuracy: 0.9547 - val_loss: 0.0706 - val_accuracy: 0.9778\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.1262 - accuracy: 0.9638 - val_loss: 0.0699 - val_accuracy: 0.9785\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.1074 - accuracy: 0.9690 - val_loss: 0.0618 - val_accuracy: 0.9813\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0943 - accuracy: 0.9740 - val_loss: 0.0590 - val_accuracy: 0.9810\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0854 - accuracy: 0.9762 - val_loss: 0.0561 - val_accuracy: 0.9821\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0809 - accuracy: 0.9767 - val_loss: 0.0530 - val_accuracy: 0.9840\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0719 - accuracy: 0.9794 - val_loss: 0.0555 - val_accuracy: 0.9828\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0649 - accuracy: 0.9813 - val_loss: 0.0446 - val_accuracy: 0.9871\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0602 - accuracy: 0.9830 - val_loss: 0.0500 - val_accuracy: 0.9852\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0540 - accuracy: 0.9842 - val_loss: 0.0468 - val_accuracy: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fade005b9a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.046773191541433334\n",
      "Test accuracy: 0.986299991607666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5-Layer CNN\n",
    "- a simple stack of 5 convolution layers with a ReLU activation and followed by max-pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN with 5 Convolutional Layers\n",
    "- This CNN takes as input tensors of shape (image_height, image_width, image_channels). In this case, I configure the CNN to process inputs of size (28, 28, 1). I do this by passing the argument input_shape=(28, 28, 1) to the first layer.\n",
    "- The Conv2D layers are used for the convolution operation that extracts features from the input images by sliding a convolution filter over the input to produce a feature map. Here I choose feature map with size 5 x 5 for the first group of the model and a feature map of 3 x 3 for the second and the third group.\n",
    "- The MaxPooling2D layers are used for the max-pooling operation that reduces the dimensionality of each feature, which helps shorten training time and reduce number of parameters. Here I choose the pooling window with size 2 x 2 for all the groups.\n",
    "- To normalize the input layers, I use the BatchNormalization layers to adjust and scale the activations. Batch Normalization reduces the amount by what the hidden unit values shift around (covariance shift). Also, it allows each layer of a network to learn by itself a little bit more independently of other layers.\n",
    "- To combat overfitting, I use the Dropout layers, a powerful regularization technique. Dropout is the method used to reduce overfitting. It forces the model to learn multiple independent representations of the same data by randomly disabling neurons in the learning phase. For example, the layers will randomly disable 40% of the outputs in all the groups.\n",
    "- My model uses 5 Conv2D layers , 2 MaxPool2D, 3 layers of BatchNormalization and 4 layers of Dropout.\n",
    "- In the end, we use Flatten() to get the image dimensionality down to 1D and add 2 Dense fully-connected layers on top. The Dense layers process 1D image vectors for our output.\n",
    "- I have done a 10-way classification as there are 10 output labels in the dataset. Softmax activation enables me to calculate the output based on the probabilities. Each class is assigned a probability and the class with the maximum probability is the model’s output for the input.\n",
    "- All the other layers, I have used “relu” activation function because “relu” improves neural network by speeding up the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=5,input_shape=(28, 28, 1), activation = 'relu'))\n",
    "model.add(Conv2D(32, kernel_size=5, activation = 'relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
    "model.add(Conv2D(64, kernel_size=3,activation = 'relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "469/469 [==============================] - 65s 139ms/step - loss: 0.3116 - accuracy: 0.9013 - val_loss: 0.0733 - val_accuracy: 0.9786\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 61s 130ms/step - loss: 0.0889 - accuracy: 0.9751 - val_loss: 0.0359 - val_accuracy: 0.9896\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 61s 130ms/step - loss: 0.0671 - accuracy: 0.9814 - val_loss: 0.0390 - val_accuracy: 0.9887\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 61s 130ms/step - loss: 0.0560 - accuracy: 0.9843 - val_loss: 0.0317 - val_accuracy: 0.9904\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 62s 132ms/step - loss: 0.0504 - accuracy: 0.9850 - val_loss: 0.0272 - val_accuracy: 0.9922\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 69s 148ms/step - loss: 0.0463 - accuracy: 0.9867 - val_loss: 0.0293 - val_accuracy: 0.9919\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 75s 160ms/step - loss: 0.0435 - accuracy: 0.9875 - val_loss: 0.0256 - val_accuracy: 0.9925\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 80s 170ms/step - loss: 0.0401 - accuracy: 0.9894 - val_loss: 0.0256 - val_accuracy: 0.9920\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 74s 159ms/step - loss: 0.0379 - accuracy: 0.9890 - val_loss: 0.0408 - val_accuracy: 0.9889\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 75s 161ms/step - loss: 0.0357 - accuracy: 0.9903 - val_loss: 0.0207 - val_accuracy: 0.9948\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 83s 177ms/step - loss: 0.0319 - accuracy: 0.9912 - val_loss: 0.0254 - val_accuracy: 0.9931\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 100s 214ms/step - loss: 0.0321 - accuracy: 0.9907 - val_loss: 0.0284 - val_accuracy: 0.9921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fade26055b0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.028447283431887627\n",
      "Test accuracy: 0.9921000003814697\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5X5filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "469/469 [==============================] - 21s 44ms/step - loss: 0.4281 - accuracy: 0.8666 - val_loss: 0.0755 - val_accuracy: 0.9761\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1434 - accuracy: 0.9592 - val_loss: 0.0500 - val_accuracy: 0.9842\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.1013 - accuracy: 0.9703 - val_loss: 0.0401 - val_accuracy: 0.9878\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0821 - accuracy: 0.9760 - val_loss: 0.0337 - val_accuracy: 0.9890\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 20s 42ms/step - loss: 0.0716 - accuracy: 0.9790 - val_loss: 0.0298 - val_accuracy: 0.9906\n",
      "Epoch 6/12\n",
      "469/469 [==============================] - 20s 42ms/step - loss: 0.0650 - accuracy: 0.9811 - val_loss: 0.0299 - val_accuracy: 0.9907\n",
      "Epoch 7/12\n",
      "469/469 [==============================] - 20s 44ms/step - loss: 0.0592 - accuracy: 0.9828 - val_loss: 0.0251 - val_accuracy: 0.9919\n",
      "Epoch 8/12\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.0523 - accuracy: 0.9853 - val_loss: 0.0302 - val_accuracy: 0.9908\n",
      "Epoch 9/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0472 - accuracy: 0.9859 - val_loss: 0.0258 - val_accuracy: 0.9918\n",
      "Epoch 10/12\n",
      "469/469 [==============================] - 19s 41ms/step - loss: 0.0451 - accuracy: 0.9870 - val_loss: 0.0261 - val_accuracy: 0.9925\n",
      "Epoch 11/12\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.0389 - accuracy: 0.9884 - val_loss: 0.0224 - val_accuracy: 0.9923\n",
      "Epoch 12/12\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.0226 - val_accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fade19c0160>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.02264484018087387\n",
      "Test accuracy: 0.9926999807357788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "gHEg94O2Xawy",
    "outputId": "d6f8e9e1-0445-403c-b862-9dd576eeb8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Epoch 1/12\n",
      "469/469 [==============================] - 143s 305ms/step - loss: 2.2857 - accuracy: 0.1107 - val_loss: 2.2550 - val_accuracy: 0.1819\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 143s 305ms/step - loss: 2.2397 - accuracy: 0.1975 - val_loss: 2.1997 - val_accuracy: 0.3695\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 143s 305ms/step - loss: 2.1814 - accuracy: 0.2905 - val_loss: 2.1270 - val_accuracy: 0.5413\n",
      "Epoch 4/12\n",
      "294/469 [=================>............] - ETA: 51s - loss: 2.1221 - accuracy: 0.3666"
     ]
    }
   ],
   "source": [
    "# Credits: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.AdaDelta,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jjw2m_63XbDO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
