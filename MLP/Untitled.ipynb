{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  label  \n",
       "0          0.4601                  0.11890    0.0  \n",
       "1          0.2750                  0.08902    0.0  \n",
       "2          0.3613                  0.08758    0.0  \n",
       "3          0.6638                  0.17300    0.0  \n",
       "4          0.2364                  0.07678    0.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_data = breast.data\n",
    "\n",
    "breast_data.shape\n",
    "\n",
    "breast_labels = breast.target\n",
    "\n",
    "breast_labels.shape\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "labels = np.reshape(breast_labels,(569,1))\n",
    "\n",
    "final_breast_data = np.concatenate([breast_data,labels],axis=1)\n",
    "\n",
    "final_breast_data.shape\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "breast_dataset = pd.DataFrame(final_breast_data)\n",
    "\n",
    "features = breast.feature_names\n",
    "\n",
    "features\n",
    "\n",
    "\n",
    "features_labels = np.append(features,'label')\n",
    "\n",
    "\n",
    "\n",
    "breast_dataset.columns = features_labels\n",
    "\n",
    "breast_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    357\n",
       "0.0    212\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_dataset['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=breast_dataset.loc[:, breast_dataset.columns != 'label']\n",
    "y=breast_dataset.label\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=101)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim =1\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "batch_size = 128 \n",
    "nb_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start building a model\n",
    "model = Sequential()\n",
    "\n",
    "# The model needs to know what input shape it should expect. \n",
    "# For this reason, the first layer in a Sequential model \n",
    "# (and only the first, because following layers can do automatic shape inference)\n",
    "# needs to receive information about its input shape. \n",
    "# you can use input_shape and input_dim to pass the shape of input\n",
    "\n",
    "# output_dim represent the number of nodes need in that layer\n",
    "# here we have 10 nodes\n",
    "\n",
    "model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n"
     ]
    }
   ],
   "source": [
    "# Before training a model, you need to configure the learning process, which is done via the compile method\n",
    "\n",
    "# It receives three arguments:\n",
    "# An optimizer. This could be the string identifier of an existing optimizer , https://keras.io/optimizers/\n",
    "# A loss function. This is the objective that the model will try to minimize., https://keras.io/losses/\n",
    "# A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'].  https://keras.io/metrics/\n",
    "\n",
    "\n",
    "# Note: when using the categorical_crossentropy loss, your targets should be in categorical format \n",
    "# (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except \n",
    "# for a 1 at the index corresponding to the class of the sample).\n",
    "\n",
    "# that is why we converted out labels into vectors\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Keras models are trained on Numpy arrays of input data and labels. \n",
    "# For training a model, you will typically use the  fit function\n",
    "\n",
    "# fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n",
    "# validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n",
    "# validation_steps=None)\n",
    "\n",
    "# fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "\n",
    "# it returns A History object. Its History.history attribute is a record of training loss values and \n",
    "# metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n",
    "\n",
    "# https://github.com/openai/baselines/issues/20\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 507us/step - loss: 5.6816 - accuracy: 0.6274\n",
      "Accuracy: 62.74\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_dynamic(x, vy, ty, ax, colors=['b']):\n",
    "    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x, ty, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 5.759020805358887\n",
      "Test accuracy: 0.6223404407501221\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('epoch') ; ax.set_ylabel('binary_crossentropy Loss')\n",
    "\n",
    "# list of epoch numbers\n",
    "x = list(range(1,nb_epoch+1))\n",
    "\n",
    "# print(history.history.keys())\n",
    "# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n",
    "# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "# we will get val_loss and val_acc only when you pass the paramter validation_data\n",
    "# val_loss : validation loss\n",
    "# val_acc : validation accuracy\n",
    "\n",
    "# loss : training loss\n",
    "# acc : train accuracy\n",
    "# for each key in histrory.histrory we will have a list of length equal to number of epochs\n",
    "\n",
    "vy = history.history['val_loss']\n",
    "ty = history.history['loss']\n",
    "plt_dynamic(x, vy, ty, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 16)                496       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 641\n",
      "Trainable params: 641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Multilayer perceptron\n",
    "\n",
    "model_sigmoid = Sequential()\n",
    "model_sigmoid.add(Dense(16, activation='sigmoid', input_shape=(input_dim,)))\n",
    "model_sigmoid.add(Dense(8, activation='sigmoid'))\n",
    "model_sigmoid.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "model_sigmoid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.6434 - accuracy: 0.6299 - val_loss: 5.7590 - val_accuracy: 0.6223\n"
     ]
    }
   ],
   "source": [
    "model_sigmoid.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
